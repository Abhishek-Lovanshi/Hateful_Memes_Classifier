{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o2KpKYf54E74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fd636ba-2e66-4360-d98e-24a7831f054f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1mMmM5Pd36Vs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bded904b-069a-4ffa-e8a5-a81c9343a653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kVTGL7lH36Vu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b7612e-26fd-456c-a564-872f8fbc2017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas_path\n",
            "  Downloading pandas_path-0.3.0-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.10/dist-packages (from pandas_path) (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas_path) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas_path) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas_path) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas_path) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.23->pandas_path) (1.16.0)\n",
            "Installing collected packages: pandas_path\n",
            "Successfully installed pandas_path-0.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BDhIUNCT36Vu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b54dbf-b4c1-4263-84c6-50782d2676f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning==0.7.5\n",
            "  Downloading pytorch_lightning-0.7.5-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==0.7.5) (4.66.2)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==0.7.5) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==0.7.5) (2.2.1+cu121)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==0.7.5) (2.15.2)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==0.7.5) (0.18.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.5) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.5) (1.62.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.5) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.5) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.5) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.5) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.5) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.5) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.5) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.5) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.5) (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->pytorch-lightning==0.7.5) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->pytorch-lightning==0.7.5) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->pytorch-lightning==0.7.5) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->pytorch-lightning==0.7.5) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->pytorch-lightning==0.7.5) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->pytorch-lightning==0.7.5) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.1->pytorch-lightning==0.7.5)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.1->pytorch-lightning==0.7.5)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.1->pytorch-lightning==0.7.5)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.1->pytorch-lightning==0.7.5)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.1->pytorch-lightning==0.7.5)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.1->pytorch-lightning==0.7.5)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.1->pytorch-lightning==0.7.5)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.1->pytorch-lightning==0.7.5)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.1->pytorch-lightning==0.7.5)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.1->pytorch-lightning==0.7.5)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.1->pytorch-lightning==0.7.5)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->pytorch-lightning==0.7.5) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.1->pytorch-lightning==0.7.5)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.7.5) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.7.5) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.7.5) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=1.14->pytorch-lightning==0.7.5) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.7.5) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.7.5) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.7.5) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.7.5) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=1.14->pytorch-lightning==0.7.5) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1->pytorch-lightning==0.7.5) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.7.5) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=1.14->pytorch-lightning==0.7.5) (3.2.2)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-lightning\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pytorch-lightning-0.7.5\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning==0.7.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "obCEflE04y9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "978cfaf8-0386-4d5b-b1c8-af4104a4fe8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4227141 sha256=fec8c15a96d1fa42ac94eaaf88d704b32fc8c64c9829db642e348ca7b534c785\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9dnQ8jJmyPog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "182a59ad-3b4a-41b6-c19e-c1f6fdcb76cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting telebot\n",
            "  Downloading telebot-0.0.5-py3-none-any.whl (4.8 kB)\n",
            "Collecting pyTelegramBotAPI (from telebot)\n",
            "  Downloading pytelegrambotapi-4.17.0-py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from telebot) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->telebot) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->telebot) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->telebot) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->telebot) (2024.2.2)\n",
            "Installing collected packages: pyTelegramBotAPI, telebot\n",
            "Successfully installed pyTelegramBotAPI-4.17.0 telebot-0.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install telebot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tkY997YMyQOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdcd2666-eee7-4f0f-c006-2e648972f8b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.17.1+cu121)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.9.0.80)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.11.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (9.4.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.3)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.4)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from python-bidi->easyocr) (1.16.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.4.18)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n",
            "Installing collected packages: pyclipper, ninja, python-bidi, easyocr\n",
            "Successfully installed easyocr-1.7.1 ninja-1.11.1.1 pyclipper-1.3.0.post5 python-bidi-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install easyocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eocLIrC836Vu"
      },
      "outputs": [],
      "source": [
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import random\n",
        "import tarfile\n",
        "import tempfile\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import fasttext\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas_path import path\n",
        "from tqdm import tqdm\n",
        "import pytorch_lightning as pl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iHttPUtf36Vv"
      },
      "outputs": [],
      "source": [
        "SEED = 29082013\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAkoGhaQ4DIT",
        "outputId": "ed1a925a-9127-4151-9d78-bee3d110dd21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import contractions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3c3TKX27zdHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e5d3c1-0709-4677-cc72-bab570783c2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import telebot\n",
        "import easyocr\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "reader = easyocr.Reader(['en'])\n",
        "# Replace with your bot token and desired image save path\n",
        "TOKEN = '6633194857:AAGFUjlf8NrFKjXVG64R3inWyR9sgYvWKKk'\n",
        "IMAGE_SAVE_PATH = '/content/drive/MyDrive/major project/data/img'\n",
        "JSON_SAVE_PATH = '/content/drive/MyDrive/major project/data'\n",
        "\n",
        "bot = telebot.TeleBot(TOKEN)\n",
        "\n",
        "@bot.message_handler(commands=['start'])\n",
        "def handle_start(message):\n",
        "    bot.reply_to(message, \"Hello! I'm a bot. How can I help you?\")\n",
        "\n",
        "@bot.message_handler(commands=['help'])\n",
        "def handle_help(message):\n",
        "    bot.reply_to(message, \"You can send me a message and I'll try to respond. I'm still learning, so please be patient with me!\")\n",
        "\n",
        "@bot.message_handler(commands=['show_text'])\n",
        "def handle_help(message):\n",
        "    photo = message.photo[-1]\n",
        "    file_info = bot.get_file(photo.file_id)\n",
        "    downloaded_file = bot.download_file(file_info.file_path)\n",
        "    filename = os.path.join(IMAGE_SAVE_PATH, f\"{photo.file_id}.png\")\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(downloaded_file)\n",
        "    result = reader.readtext(downloaded_file)\n",
        "    # Extract and format the text\n",
        "    text = '\\n'.join([item[1] for item in result])\n",
        "    bot.reply_to(message, text)\n",
        "\n",
        "@bot.message_handler(content_types=['photo'])\n",
        "def photo(message):\n",
        "    photo = message.photo[-1]\n",
        "    file_info = bot.get_file(photo.file_id)\n",
        "    downloaded_file = bot.download_file(file_info.file_path)\n",
        "    # Perform OCR using easyocr\n",
        "    filename = os.path.join(IMAGE_SAVE_PATH, f\"{photo.file_id}.png\")\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(downloaded_file)\n",
        "    result = reader.readtext(downloaded_file)\n",
        "    # Extract and format the text\n",
        "    # text = '\\n'.join([item[1] for item in result])\n",
        "    text = ' '.join([item[1] for item in result])\n",
        "    text = text.strip()\n",
        "    print(text)\n",
        "    data = {\n",
        "        \"id\":16395,\"img\":f\"img/{photo.file_id}.png\",\"text\":text\n",
        "    }\n",
        "    jsonl_filename = os.path.join(JSON_SAVE_PATH, f\"{photo.file_id}.jsonl\")\n",
        "    with open(jsonl_filename, 'w', newline='\\n') as jsonl_file:\n",
        "        json.dump(data, jsonl_file)\n",
        "    data_dir = '/content/drive/MyDrive/major project/data/'\n",
        "    img_path = data_dir + \"data_dir\"\n",
        "    test_path = jsonl_filename\n",
        "    train_path = data_dir + \"train.jsonl\"\n",
        "    dev_path = data_dir + \"dev.jsonl\"\n",
        "    print(train_path)\n",
        "    print(test_path)\n",
        "    print(img_path)\n",
        "    test_samples_frame = pd.read_json(test_path, lines=True)\n",
        "    print(test_samples_frame.shape)\n",
        "    test_samples_frame.head()\n",
        "\n",
        "    from PIL import Image\n",
        "    image = Image.open(\n",
        "            filename\n",
        "        ).convert(\"RGB\")\n",
        "    print(image.size)\n",
        "    # define a callable image_transform with Compose\n",
        "    image_transform = torchvision.transforms.Compose(\n",
        "        [\n",
        "            torchvision.transforms.Resize(size=(224, 224)),\n",
        "            torchvision.transforms.ToTensor()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # convert the images and prepare for visualization.\n",
        "    tensor_img = torch.stack(\n",
        "        [image_transform(image)]\n",
        "    )\n",
        "    grid = torchvision.utils.make_grid(tensor_img)\n",
        "\n",
        "    # plot\n",
        "    plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
        "    plt.axis('off')\n",
        "    _ = plt.imshow(grid.permute(1, 2, 0))\n",
        "    from unicodedata import normalize, category\n",
        "\n",
        "    def clean_text_1(_text):\n",
        "        _text = contractions.fix(_text).lower().replace('(', '( ').replace(')', ' ) ').replace('\\n', '')\n",
        "\n",
        "        selects_characters = ['Ll', 'Zs', 'Lu'] #, 'Nd'\n",
        "        return ''.join(\n",
        "            [_ for _ in normalize('NFD', _text) if category(_) in selects_characters]\n",
        "        ).split()\n",
        "\n",
        "    def quit_stopwords(_text):\n",
        "        return ' '.join([word for word in _text if word not in stop_words and len(word) > 2])\n",
        "    stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "    stop_words = list(set(stop_words + contractions.fix(' '.join(stop_words)).lower().split()))\n",
        "    print(stop_words)\n",
        "    value = np.random.choice([0,1])\n",
        "\n",
        "\n",
        "    class HatefulMemesDataset(torch.utils.data.Dataset):\n",
        "        def __init__(\n",
        "            self,\n",
        "            data_path,\n",
        "            img_dir,\n",
        "            image_transform,\n",
        "            text_transform,\n",
        "            balance=False,\n",
        "            dev_limit=None,\n",
        "            random_state=SEED,\n",
        "        ):\n",
        "\n",
        "            self.samples_frame = pd.read_json(\n",
        "                data_path, lines=True\n",
        "            )\n",
        "            self.dev_limit = dev_limit\n",
        "            if balance:\n",
        "                neg = self.samples_frame[\n",
        "                    self.samples_frame.label.eq(0)\n",
        "                ]\n",
        "                pos = self.samples_frame[\n",
        "                    self.samples_frame.label.eq(1)\n",
        "                ]\n",
        "                self.samples_frame = pd.concat(\n",
        "                    [\n",
        "                        neg.sample(\n",
        "                            pos.shape[0],\n",
        "                            random_state=random_state\n",
        "                        ),\n",
        "                        pos\n",
        "                    ]\n",
        "                )\n",
        "            if self.dev_limit:\n",
        "                if self.samples_frame.shape[0] > self.dev_limit:\n",
        "                    self.samples_frame = self.samples_frame.sample(\n",
        "                        dev_limit, random_state=random_state\n",
        "                    )\n",
        "            self.samples_frame = self.samples_frame.reset_index(\n",
        "                drop=True\n",
        "            )\n",
        "            self.samples_frame.img = self.samples_frame.apply(\n",
        "                lambda row: (img_dir + row.img), axis=1\n",
        "            )\n",
        "            print(\"Image paths:\")\n",
        "            print(self.samples_frame.img)\n",
        "            # Check if image files exist\n",
        "            invalid_paths = self.samples_frame[~self.samples_frame.img.apply(lambda x: os.path.exists(x))]\n",
        "            if not invalid_paths.empty:\n",
        "                # Remove extra backslashes from paths\n",
        "                self.samples_frame.img = self.samples_frame.img.str.replace(r'\\\\', '/')\n",
        "                # Recheck for invalid paths\n",
        "                invalid_paths = self.samples_frame[~self.samples_frame.img.apply(lambda x: os.path.exists(x))]\n",
        "                if not invalid_paths.empty:\n",
        "                    print(\"Invalid image paths:\")\n",
        "                    print(invalid_paths.img)\n",
        "                    raise FileNotFoundError(\"One or more image files do not exist.\")\n",
        "\n",
        "            self.image_transform = image_transform\n",
        "            self.text_transform = text_transform\n",
        "\n",
        "        def __len__(self):\n",
        "            \"\"\"This method is called when you do len(instance)\n",
        "            for an instance of this class.\n",
        "            \"\"\"\n",
        "            return len(self.samples_frame)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            \"\"\"This method is called when you do instance[key]\n",
        "            for an instance of this class.\n",
        "            \"\"\"\n",
        "            if torch.is_tensor(idx):\n",
        "                idx = idx.tolist()\n",
        "\n",
        "            img_id = self.samples_frame.loc[idx, \"id\"]\n",
        "\n",
        "            image = Image.open(\n",
        "                self.samples_frame.loc[idx, \"img\"]\n",
        "            ).convert(\"RGB\")\n",
        "            image = self.image_transform(image)\n",
        "\n",
        "            text = torch.Tensor(\n",
        "                self.text_transform.get_sentence_vector(\n",
        "                    self.samples_frame.loc[idx, \"text\"]\n",
        "                )\n",
        "            ).squeeze()\n",
        "\n",
        "            if \"label\" in self.samples_frame.columns:\n",
        "                label = torch.Tensor(\n",
        "                    [self.samples_frame.loc[idx, \"label\"]]\n",
        "                ).long().squeeze()\n",
        "                sample = {\n",
        "                    \"id\": img_id,\n",
        "                    \"image\": image,\n",
        "                    \"text\": text,\n",
        "                    \"label\": label\n",
        "                }\n",
        "            else:\n",
        "                sample = {\n",
        "                    \"id\": img_id,\n",
        "                    \"image\": image,\n",
        "                    \"text\": text\n",
        "                }\n",
        "\n",
        "            return sample\n",
        "\n",
        "    class LanguageAndVisionConcat(torch.nn.Module):\n",
        "        def __init__(\n",
        "            self,\n",
        "            num_classes,\n",
        "            loss_fn,\n",
        "            language_module,\n",
        "            vision_module,\n",
        "            language_feature_dim,\n",
        "            vision_feature_dim,\n",
        "            fusion_output_size,\n",
        "            dropout_p,\n",
        "\n",
        "        ):\n",
        "            super(LanguageAndVisionConcat, self).__init__()\n",
        "            self.language_module = language_module\n",
        "            self.vision_module = vision_module\n",
        "\n",
        "            print(language_feature_dim)\n",
        "            print(vision_feature_dim)\n",
        "            assert language_feature_dim == vision_feature_dim\n",
        "\n",
        "            self.fusion = torch.nn.Linear(\n",
        "                in_features=vision_feature_dim,\n",
        "                out_features=fusion_output_size\n",
        "            )\n",
        "            self.fc = torch.nn.Linear(\n",
        "                in_features=fusion_output_size,\n",
        "                out_features=num_classes\n",
        "            )\n",
        "            self.loss_fn = loss_fn\n",
        "            self.dropout = torch.nn.Dropout(dropout_p)\n",
        "\n",
        "        def forward(self, text, image, label=None):\n",
        "            text_features = torch.nn.functional.relu(\n",
        "                self.language_module(text)\n",
        "            )\n",
        "            image_features = torch.nn.functional.relu(\n",
        "                self.vision_module(image)\n",
        "            )\n",
        "\n",
        "\n",
        "            combined = torch.mul(text_features, image_features)\n",
        "\n",
        "            fused = self.dropout(\n",
        "                torch.nn.functional.relu(\n",
        "                self.fusion(combined)\n",
        "                )\n",
        "            )\n",
        "            logits = self.fc(fused)\n",
        "            pred = torch.nn.functional.softmax(logits)\n",
        "            loss = (\n",
        "                self.loss_fn(pred, label)\n",
        "                if label is not None else label\n",
        "            )\n",
        "            return (pred, loss)\n",
        "    x = torch.randn(2, 3)\n",
        "    # for the purposes of this post, we'll filter\n",
        "    # much of the lovely logging info from our LightningModule\n",
        "    #warnings.filterwarnings(\"ignore\")\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    class HatefulMemesModel(pl.LightningModule):\n",
        "        def __init__(self, hparams):\n",
        "            for data_key in [\"train_path\", \"dev_path\", \"img_dir\",]:\n",
        "                # ok, there's one for-loop but it doesn't count\n",
        "                if data_key not in hparams.keys():\n",
        "                    raise KeyError(\n",
        "                        f\"{data_key} is a required hparam in this model\"\n",
        "                    )\n",
        "\n",
        "            super(HatefulMemesModel, self).__init__()\n",
        "            self.hparams = hparams\n",
        "\n",
        "            # assign some hparams that get used in multiple places\n",
        "            self.embedding_dim = self.hparams.get(\n",
        "                \"embedding_dim\", 500\n",
        "            )\n",
        "            self.language_feature_dim = self.hparams.get(\n",
        "                \"language_feature_dim\", 500\n",
        "            )\n",
        "            self.vision_feature_dim = self.hparams.get(\n",
        "                \"vision_feature_dim\", 500\n",
        "            )\n",
        "            self.output_path = Path(\n",
        "                self.hparams.get(\"output_path\", \"model-outputs\")\n",
        "            )\n",
        "            self.output_path.mkdir(exist_ok=True)\n",
        "\n",
        "            # instantiate transforms, datasets\n",
        "            self.text_transform = self._build_text_transform()\n",
        "            self.image_transform = self._build_image_transform()\n",
        "            self.train_dataset = self._build_dataset(\"train_path\")\n",
        "            self.dev_dataset = self._build_dataset(\"dev_path\")\n",
        "\n",
        "            # set up model and training\n",
        "            self.model = self._build_model()\n",
        "            self.trainer_params = self._get_trainer_params()\n",
        "\n",
        "\n",
        "        def forward(self, text, image, label=None):\n",
        "            return self.model(text, image, label)\n",
        "\n",
        "        def training_step(self, batch, batch_nb):\n",
        "            preds, loss = self.forward(\n",
        "                text=batch[\"text\"],\n",
        "                image=batch[\"image\"],\n",
        "                label=batch[\"label\"]\n",
        "            )\n",
        "\n",
        "            return {\"loss\": loss}\n",
        "\n",
        "        def validation_step(self, batch, batch_nb):\n",
        "            preds, loss = self.eval().forward(\n",
        "                text=batch[\"text\"],\n",
        "                image=batch[\"image\"],\n",
        "                label=batch[\"label\"]\n",
        "            )\n",
        "\n",
        "            return {\"batch_val_loss\": loss}\n",
        "\n",
        "        def validation_epoch_end(self, outputs):\n",
        "            avg_loss = torch.stack(\n",
        "                tuple(\n",
        "                    output[\"batch_val_loss\"]\n",
        "                    for output in outputs\n",
        "                )\n",
        "            ).median()\n",
        "\n",
        "            return {\n",
        "                \"val_loss\": avg_loss,\n",
        "                \"progress_bar\":{\"avg_val_loss\": avg_loss}\n",
        "            }\n",
        "\n",
        "        def configure_optimizers(self):\n",
        "            optimizers = [\n",
        "                torch.optim.AdamW(\n",
        "                    self.model.parameters(),\n",
        "                    lr=self.hparams.get(\"lr\", 0.001)\n",
        "                )\n",
        "            ]\n",
        "            schedulers = [\n",
        "                torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                    optimizers[0],\n",
        "                    factor=0.1,\n",
        "                    patience=2\n",
        "                )\n",
        "            ]\n",
        "            return optimizers, schedulers\n",
        "\n",
        "        @pl.data_loader\n",
        "        def train_dataloader(self):\n",
        "            return torch.utils.data.DataLoader(\n",
        "                self.train_dataset,\n",
        "                shuffle=True,\n",
        "                batch_size=self.hparams.get(\"batch_size\", 4),\n",
        "                num_workers=self.hparams.get(\"num_workers\", 16)\n",
        "            )\n",
        "\n",
        "        @pl.data_loader\n",
        "        def val_dataloader(self):\n",
        "            return torch.utils.data.DataLoader(\n",
        "                self.dev_dataset,\n",
        "                shuffle=False,\n",
        "                batch_size=self.hparams.get(\"batch_size\", 4),\n",
        "                num_workers=self.hparams.get(\"num_workers\", 16)\n",
        "            )\n",
        "\n",
        "        ## Convenience Methods ##\n",
        "\n",
        "        def fit(self):\n",
        "            self._set_seed(self.hparams.get(\"random_state\", SEED))\n",
        "            self.trainer = pl.Trainer(**self.trainer_params)\n",
        "            self.trainer.fit(self)\n",
        "\n",
        "        def _set_seed(self, seed=SEED):\n",
        "            random.seed(seed)\n",
        "            np.random.seed(seed)\n",
        "\n",
        "\n",
        "            torch.manual_seed(seed)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.manual_seed_all(seed)\n",
        "        def _build_text_transform(self):\n",
        "            with tempfile.NamedTemporaryFile() as ft_training_data:\n",
        "                ft_path = Path(ft_training_data.name)\n",
        "                with ft_path.open(\"w\") as ft:\n",
        "                    training_data = [\n",
        "                        quit_stopwords(clean_text_1(json.loads(line)[\"text\"])) + \"/n\"\n",
        "                        for line in open(\n",
        "                            self.hparams.get(\"train_path\")\n",
        "                        ).read().splitlines()\n",
        "                    ]\n",
        "                    for line in training_data:\n",
        "                        ft.write(line + \"\\n\")\n",
        "                    language_transform = fasttext.train_unsupervised(\n",
        "                        str(ft_path),\n",
        "                        model=self.hparams.get(\"fasttext_model\", \"skipgram\"),\n",
        "                        dim=self.embedding_dim,\n",
        "                        epoch=20,\n",
        "                        lr=0.001\n",
        "                    )\n",
        "            return language_transform\n",
        "\n",
        "        def _build_image_transform(self):\n",
        "            image_dim = self.hparams.get(\"image_dim\", 224)\n",
        "            image_transform = torchvision.transforms.Compose(\n",
        "                [\n",
        "                    torchvision.transforms.Resize(\n",
        "                        size=(image_dim, image_dim)\n",
        "                    ),\n",
        "                    torchvision.transforms.ToTensor(),\n",
        "                    torchvision.transforms.Normalize(\n",
        "                        mean=(0.485, 0.456, 0.406),\n",
        "                        std=(0.229, 0.224, 0.225)\n",
        "                    ),\n",
        "                ]\n",
        "            )\n",
        "            return image_transform\n",
        "\n",
        "        def _build_dataset(self, dataset_key):\n",
        "            return HatefulMemesDataset(\n",
        "                data_path=self.hparams.get(dataset_key, dataset_key),\n",
        "                img_dir=self.hparams.get(\"img_dir\"),\n",
        "                image_transform=self.image_transform,\n",
        "                text_transform=self.text_transform,\n",
        "                # limit training samples only\n",
        "                dev_limit=(\n",
        "                    self.hparams.get(\"dev_limit\", None)\n",
        "                    if \"train\" in str(dataset_key) else None\n",
        "                ),\n",
        "                balance=True if \"train\" in str(dataset_key) else False,\n",
        "            )\n",
        "\n",
        "        def _build_model(self):\n",
        "\n",
        "            language_module = torch.nn.Linear(\n",
        "                    in_features=self.embedding_dim,\n",
        "                    out_features=self.language_feature_dim\n",
        "            )\n",
        "\n",
        "\n",
        "            vision_module = torchvision.models.resnet152(\n",
        "                pretrained=True\n",
        "            )\n",
        "            vision_module.fc = torch.nn.Linear(\n",
        "                    in_features=2048,\n",
        "                    out_features=self.vision_feature_dim\n",
        "            )\n",
        "\n",
        "            return LanguageAndVisionConcat(\n",
        "                num_classes=self.hparams.get(\"num_classes\", 2),\n",
        "                loss_fn=torch.nn.CrossEntropyLoss(),\n",
        "                language_module=language_module,\n",
        "                vision_module=vision_module,\n",
        "                language_feature_dim=self.language_feature_dim,\n",
        "                vision_feature_dim=self.vision_feature_dim,\n",
        "                fusion_output_size=self.hparams.get(\n",
        "                    \"fusion_output_size\", 512\n",
        "                ),\n",
        "                dropout_p=self.hparams.get(\"dropout_p\", 0.1),\n",
        "            )\n",
        "\n",
        "        def _get_trainer_params(self):\n",
        "            checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "                filepath=self.output_path,\n",
        "                monitor=self.hparams.get(\n",
        "                    \"checkpoint_monitor\", \"avg_val_loss\"\n",
        "                ),\n",
        "                mode=self.hparams.get(\n",
        "                    \"checkpoint_monitor_mode\", \"min\"\n",
        "                ),\n",
        "                verbose=self.hparams.get(\"verbose\", True)\n",
        "            )\n",
        "\n",
        "            early_stop_callback = pl.callbacks.EarlyStopping(\n",
        "                monitor=self.hparams.get(\n",
        "                    \"early_stop_monitor\", \"avg_val_loss\"\n",
        "                ),\n",
        "                min_delta=self.hparams.get(\n",
        "                    \"early_stop_min_delta\", 0.0001\n",
        "                ),\n",
        "                patience=self.hparams.get(\n",
        "                    \"early_stop_patience\", 5\n",
        "                ),\n",
        "                verbose=self.hparams.get(\"verbose\", True),\n",
        "            )\n",
        "\n",
        "            trainer_params = {\n",
        "                \"checkpoint_callback\": checkpoint_callback,\n",
        "                \"early_stop_callback\": early_stop_callback,\n",
        "                \"default_save_path\": self.output_path,\n",
        "                \"accumulate_grad_batches\": self.hparams.get(\n",
        "                    \"accumulate_grad_batches\", 1\n",
        "                ),\n",
        "                \"gpus\": self.hparams.get(\"n_gpu\", 1),\n",
        "                \"max_epochs\": self.hparams.get(\"max_epochs\", 100),\n",
        "                \"gradient_clip_val\": self.hparams.get(\n",
        "                    \"gradient_clip_value\", 1\n",
        "                ),\n",
        "            }\n",
        "            return trainer_params\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def make_submission_frame(self, test_path):\n",
        "            test_dataset = self._build_dataset(test_path)\n",
        "            print(test_dataset)\n",
        "            print(type(test_dataset))\n",
        "            print(dir(test_dataset))\n",
        "            print(\"/\"*100)\n",
        "            print(dir(test_dataset.samples_frame))\n",
        "            print(\"/\"*100)\n",
        "            print(test_dataset.samples_frame.id)\n",
        "            print(type(test_dataset.samples_frame.id))\n",
        "\n",
        "            submission_frame = pd.DataFrame(\n",
        "                index=test_dataset.samples_frame.id,\n",
        "                columns=[\"proba\", \"label_predict\"]\n",
        "            )\n",
        "            # print(submission_frame.head(10))\n",
        "\n",
        "            test_dataloader = torch.utils.data.DataLoader(\n",
        "                test_dataset,\n",
        "                shuffle=False,\n",
        "                batch_size=self.hparams.get(\"batch_size\", 4),\n",
        "                num_workers=self.hparams.get(\"num_workers\", 16))\n",
        "\n",
        "            for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n",
        "\n",
        "                preds, _ = self.model.eval()(\n",
        "                    batch[\"text\"], batch[\"image\"]\n",
        "                )\n",
        "                submission_frame.loc[batch[\"id\"], \"proba\"] = preds[:, 1]\n",
        "                submission_frame.loc[batch[\"id\"], \"label_predict\"] = preds.argmax(dim=1)\n",
        "\n",
        "            submission_frame.proba = submission_frame.proba.astype(float)\n",
        "            submission_frame.label_predict = submission_frame.label_predict.astype(int)\n",
        "            return submission_frame\n",
        "    checkpoints = list(Path(\"/content/drive/MyDrive/\").glob(\"*.ckpt\"))\n",
        "    assert len(checkpoints) == 1\n",
        "    checkpoints\n",
        "    hateful_memes_model = HatefulMemesModel.load_from_checkpoint(\n",
        "        checkpoints[0]\n",
        "    )\n",
        "    submission = hateful_memes_model.make_submission_frame(\n",
        "        test_path\n",
        "    )\n",
        "    if ((submission['label_predict'] == 1) & (submission['proba'] > 0.5)).any() or ((submission['label_predict'] == 0) & (submission['proba'] < 0.4)).any():\n",
        "        bot.reply_to(message, \"Hateful\")\n",
        "    else:\n",
        "        bot.reply_to(message, \"Benign\")\n",
        "    print(submission)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocQylcRQ05ZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f84e9e49-acf1-4b04-a852-1c4041c7f929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X a smile iS worth a tousand words\n",
            "/content/drive/MyDrive/major project/data/train.jsonl\n",
            "/content/drive/MyDrive/major project/data/AgACAgUAAx0CYA6UGwAD_2YwcFNeTmlTxnSshEaY1l0XHEC9AAIhwDEbTN-ZVA307R7Pr2ScAQADAgADeQADNAQ.jsonl\n",
            "/content/drive/MyDrive/major project/data/data_dir\n",
            "(1, 3)\n",
            "(1080, 714)\n",
            "['own', 'be', 'because', 'same', 'of', 'might', 'for', 'wouldn', 'such', 'shall', \"should've\", 'the', 'nor', 'their', 're', 'before', 'what', 'll', \"don't\", \"didn't\", 'its', 'if', \"hadn't\", 'no', 'as', 'weren', 'some', 'he', 'by', 'all', 'above', 'it', 'at', 'yours', 'your', 'i', \"isn't\", 'both', 'him', \"weren't\", 'after', 'should', 'between', 'could', 'herself', 'this', 'now', 'was', 'haven', 'you', 'that', 'been', 'an', 'how', 'to', 'down', 'himself', 'why', 'were', 'who', \"you're\", 'out', 'yourself', 'so', 'these', 'has', 'me', 'in', 'my', 'have', 'once', \"she's\", 'aren', 'too', 'need', 'had', \"wouldn't\", 'other', 'shouldn', \"hasn't\", 'each', 'which', 'needn', 'whom', 'very', 'only', 'o', 'under', 'until', \"shan't\", 'they', 'won', 'do', 'doing', 'when', \"you've\", 'didn', 'doesn', 'there', 'few', 'but', 'will', 'ma', 'hasn', \"mustn't\", 'mightn', 'through', 'would', 'while', 'most', 'his', 'ours', 'hers', 'can', 'then', 'm', 'she', \"haven't\", 'ourselves', 'again', 'must', 'did', 'and', 'with', 'is', 'yourselves', 'into', 'are', \"that'll\", 'y', 'itself', 'during', \"won't\", \"mightn't\", 'theirs', 'am', \"wasn't\", 'up', 'we', 'more', 've', 'isn', 'myself', \"it's\", 'don', 'over', 'where', 'any', 'd', 'further', 'having', 'wasn', 'here', 't', \"you'd\", 'or', 'a', 'about', 'ain', 'below', 's', \"shouldn't\", 'shan', 'our', \"you'll\", 'not', \"doesn't\", 'her', 'those', 'being', 'from', 'themselves', 'just', 'hadn', 'mustn', 'them', 'couldn', \"aren't\", 'on', 'against', 'than', 'off', 'does', \"couldn't\", \"needn't\"]\n",
            "Image paths:\n",
            "0       /content/drive/MyDrive/major project/data/img/...\n",
            "1       /content/drive/MyDrive/major project/data/img/...\n",
            "2       /content/drive/MyDrive/major project/data/img/...\n",
            "3       /content/drive/MyDrive/major project/data/img/...\n",
            "4       /content/drive/MyDrive/major project/data/img/...\n",
            "                              ...                        \n",
            "6095    /content/drive/MyDrive/major project/data/img/...\n",
            "6096    /content/drive/MyDrive/major project/data/img/...\n",
            "6097    /content/drive/MyDrive/major project/data/img/...\n",
            "6098    /content/drive/MyDrive/major project/data/img/...\n",
            "6099    /content/drive/MyDrive/major project/data/img/...\n",
            "Name: img, Length: 6100, dtype: object\n"
          ]
        }
      ],
      "source": [
        "bot.polling()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzTSmdgz36V-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 715500,
          "sourceId": 1246182,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30162,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}